{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe6e8b87",
   "metadata": {},
   "source": [
    "# XGBoost on GPU, no categories, with dates, including \"onetimers\"\n",
    "## Predicting the time difference to the next repurchase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01cce02",
   "metadata": {},
   "source": [
    "### Methods & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c992df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 250)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "####\n",
    "# prints memory usage\n",
    "def show_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB\\n'.format(start_mem))\n",
    "    return\n",
    "\n",
    "####\n",
    "# seperates features from label (y must be last column)\n",
    "def sep_X_y(df_train, df_test):\n",
    "    X_train = df_train.iloc[:,0:-1] # extracts all rows [:] and columns from 0 to next-to-last [0:-1]\n",
    "    y_train = df_train.iloc[:,-1] # extracts all rows [:] and only last column [-1]\n",
    "    X_test = df_test.iloc[:,0:-1]\n",
    "    y_test = df_test.iloc[:,-1]\n",
    "    \n",
    "    return [X_train, y_train, X_test, y_test]\n",
    "\n",
    "####\n",
    "# split training and test set from given dataframe with dates as boundaries\n",
    "def dt_train_test_split(df, dt_start_train, dt_end_train, dt_start_test, dt_end_test):\n",
    "    print('Splitting dataframe...\\n')\n",
    "    \n",
    "    # get indices from desired boundaries\n",
    "    idx_start_train = df.date.searchsorted(pd.to_datetime(dt_start_train), side='left') # list needs to be sorted already for searchsorted\n",
    "    idx_end_train = df.date.searchsorted(pd.to_datetime(dt_end_train) + pd.Timedelta(days=1), side='left')\n",
    "    idx_start_test = df.date.searchsorted(pd.to_datetime(dt_start_test), side='left')\n",
    "    idx_end_test = df.date.searchsorted(pd.to_datetime(dt_end_test) + pd.Timedelta(days=1), side='left')\n",
    "    \n",
    "    train = df.iloc[idx_start_train:idx_end_train]\n",
    "    test = df.iloc[idx_start_test:idx_end_test]\n",
    "    \n",
    "    train.drop(columns=['date'], axis=0, inplace=True)\n",
    "    test.drop(columns=['date'], axis=0, inplace=True)\n",
    "    \n",
    "    return sep_X_y(train, test)\n",
    "\n",
    "####\n",
    "# trains XGB model (classifier)\n",
    "def train_xgb(X, y):\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Fitting model...\\n')\n",
    "    model = XGBRegressor(tree_method='gpu_hist', gpu_id=0)\n",
    "    fitted_model = model.fit(X_train, y_train)\n",
    "    \n",
    "    print('Plotting feature importance for \"gain\". Do not rely on that.\\n')\n",
    "    print('https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27\\n')\n",
    "    xgb.plot_importance(model, importance_type='gain')\n",
    "    plt.show()\n",
    "    \n",
    "    # GRAPHVIZ (software + pip package) needed for tree plotting\n",
    "    #fig, ax = plt.subplots(figsize=(30, 30))\n",
    "    #xgb.plot_tree(model, num_trees=0, ax=ax, rankdir='LR')\n",
    "    #plt.show()\n",
    "    \n",
    "    return fitted_model\n",
    "\n",
    "def train_xgb_bestHyper(X, y):\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    space = best_hyperparams\n",
    "    \n",
    "    print('Fitting model...\\n')\n",
    "    model = XGBRegressor(tree_method='gpu_hist', gpu_id=0,\n",
    "                    eta = space['eta'],\n",
    "                    max_depth = int(space['max_depth']), \n",
    "                    gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),\n",
    "                    min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree']))\n",
    "\n",
    "    \n",
    "    \n",
    "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    fitted_model.fit(X_train, y_train,\n",
    "            eval_set=evaluation, eval_metric=\"rmse\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "    \n",
    "    print('Plotting feature importance for \"gain\". Do not rely on that.\\n')\n",
    "    print('https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27\\n')\n",
    "    xgb.plot_importance(model, importance_type='gain')\n",
    "    plt.show()\n",
    "    \n",
    "    # GRAPHVIZ (software + pip package) needed for tree plotting\n",
    "    #fig, ax = plt.subplots(figsize=(30, 30))\n",
    "    #xgb.plot_tree(model, num_trees=0, ax=ax, rankdir='LR')\n",
    "    #plt.show()\n",
    "    \n",
    "    return fitted_model\n",
    "\n",
    "def train_dtc(X, y):\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    print('Fitting model...\\n')\n",
    "    model = DecisionTreeRegressor()\n",
    "    fitted_model = model.fit(X_train, y_train)\n",
    "    \n",
    "    print('Plotting feature importance for \"gain\". Do not rely on that.\\n')\n",
    "    print('https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27\\n')\n",
    "    #xgb.plot_importance(model, importance_type='gain')\n",
    "    #plt.show()\n",
    "    \n",
    "    # GRAPHVIZ (software + pip package) needed for tree plotting\n",
    "    #fig, ax = plt.subplots(figsize=(30, 30))\n",
    "    #xgb.plot_tree(model, num_trees=0, ax=ax, rankdir='LR')\n",
    "    #plt.show()\n",
    "    \n",
    "    return fitted_model\n",
    "\n",
    "####\n",
    "# predicts labels of training and test with given model\n",
    "def predict_values(model, X_train, y_train, X_test, y_test):\n",
    "    print('Predicting values...\\n')\n",
    "    # predict y values\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    \n",
    "    # get accuracies\n",
    "    model_train = mean_squared_error(y_train, y_train_pred)\n",
    "    model_test = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #get precision\n",
    "    model_train_precision = precision_score(y_train, y_train_pred, average)\n",
    "    model_test_precision = precision_score(y_test, y_test_pred)\n",
    "    \"\"\"\n",
    "    # print info about accuracies\n",
    "    print(f'\\n XGboost train/test rmse: '\n",
    "         f'{model_train:.3f}/{model_test:.3f}')\n",
    "    \n",
    "    # return predicted values\n",
    "    return [y_train_pred, y_test_pred]\n",
    "\n",
    "def predict_values_train(model, X_train, y_train):\n",
    "    print('Predicting values...\\n')\n",
    "    # predict y values\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # get accuracies\n",
    "    model_train = mean_squared_error(y_train, y_train_pred)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #get precision\n",
    "    model_train_precision = precision_score(y_train, y_train_pred, average)\n",
    "    model_test_precision = precision_score(y_test, y_test_pred)\n",
    "    \"\"\"\n",
    "    # print info about accuracies\n",
    "    print(f'\\n XGboost train'\n",
    "         f'{model_train:.3f}')\n",
    "    \n",
    "    # return predicted values\n",
    "    return y_train_pred\n",
    "\n",
    "####\n",
    "# concatenates prediction with actual target for evaluation\n",
    "def evaluate_pred(X, y, y_pred):\n",
    "    # create dataframe from test-prediction with index from X_test\n",
    "    df_y_pred = pd.DataFrame(y_pred, columns=['nextBuyIn_pred'], index=X.index, dtype=np.int8)\n",
    "\n",
    "    # concatenate X, y, y_pred (put columns next to each other)\n",
    "    df_eval = pd.concat([X, y, df_y_pred], axis=1)\n",
    "    \n",
    "    return df_eval\n",
    "\n",
    "####\n",
    "# executes all needed functions of the above with given training and test data and provided train method\n",
    "def execute_pipeline(train_method, df, list_of_four_df_boundaries):\n",
    "    b = list_of_four_df_boundaries\n",
    "    # split dataframe in train/test and X/y\n",
    "    X_train, y_train, X_test, y_test = dt_train_test_split(df, b[0], b[1], b[2], b[3])\n",
    "    \n",
    "    #train model\n",
    "    model = train_method(X_train, y_train)    \n",
    "    \n",
    "    # make predictions\n",
    "    pred_train, pred_test = predict_values(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print('\\nExecuted pipeline.\\nEvaluate with \"evaluate_pred(X, y, y_pred)\"\\n')\n",
    "    return [pred_train, pred_test, X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639f3776",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = r'C:\\Users\\LEAND\\Coding\\Jupyter Notebooks\\csv\\220623_complete_feature-list_orderhistory_trainingOhneNull.csv' # scores calculated with data from jun-oct\n",
    "\n",
    "columns = [#'date',\n",
    "           'userID', \n",
    "           'itemID',\n",
    "           'order', \n",
    "           'brand', \n",
    "           'feature_1', \n",
    "           'feature_2', \n",
    "           'feature_3', \n",
    "           'feature_4', \n",
    "           'feature_5',\n",
    "           'brandOrderRatio',\n",
    "           'feature1OrderRatio',\n",
    "           'feature2OrderRatio',\n",
    "           'feature3OrderRatio',\n",
    "           'feature4OrderRatio',\n",
    "           'feature5OrderRatio',\n",
    "           'TotalBFscore',\n",
    "           'RCP',\n",
    "           'MeanDiffToNxt(user)',\n",
    "           'TotalItemOrders(user)',\n",
    "           #'TotalItemOrders(item)',\n",
    "           'date(year)',\n",
    "           'date(month)',\n",
    "           'date(weekOfMonth)',\n",
    "           'date(dayOfMonth)',\n",
    "           'date(weekOfYear)',\n",
    "           'date(dayOfYear)',\n",
    "           #'nextBuyInWeeks(round)', # label\n",
    "           'nextBuyInWeeks(floor)', # label\n",
    "           #'nextBuyInWeekOfYear' # label; schlechte idee\n",
    "          ]\n",
    "\n",
    "dtype = {'userID':np.uint16,\n",
    "         'itemID':np.uint16,\n",
    "         'order':np.uint8,\n",
    "         'brand':np.int16,\n",
    "         'feature_1':np.int8,\n",
    "         'feature_2':np.uint8,\n",
    "         'feature_3':np.int16,\n",
    "         'feature_4':np.int8,\n",
    "         'feature_5':np.int16}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e4b1d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae906709",
   "metadata": {},
   "source": [
    "# Predicting Weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b34ddc",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f724795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 24.55 MB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>order</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>brandOrderRatio</th>\n",
       "      <th>feature1OrderRatio</th>\n",
       "      <th>feature2OrderRatio</th>\n",
       "      <th>feature3OrderRatio</th>\n",
       "      <th>feature4OrderRatio</th>\n",
       "      <th>feature5OrderRatio</th>\n",
       "      <th>TotalBFscore</th>\n",
       "      <th>RCP</th>\n",
       "      <th>TotalItemOrders(user)</th>\n",
       "      <th>MeanDiffToNxt(user)</th>\n",
       "      <th>date(year)</th>\n",
       "      <th>date(month)</th>\n",
       "      <th>date(weekOfMonth)</th>\n",
       "      <th>date(dayOfMonth)</th>\n",
       "      <th>date(weekOfYear)</th>\n",
       "      <th>date(dayOfYear)</th>\n",
       "      <th>nextBuyInWeeks(floor)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76</td>\n",
       "      <td>23050</td>\n",
       "      <td>1</td>\n",
       "      <td>1411</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.008540</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.018705</td>\n",
       "      <td>0.940897</td>\n",
       "      <td>0.259374</td>\n",
       "      <td>2</td>\n",
       "      <td>169.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>153</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>9408</td>\n",
       "      <td>1</td>\n",
       "      <td>322</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.034208</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.086085</td>\n",
       "      <td>0.988336</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>3</td>\n",
       "      <td>114.5</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>153</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>25677</td>\n",
       "      <td>1</td>\n",
       "      <td>322</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.034208</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.086085</td>\n",
       "      <td>0.988336</td>\n",
       "      <td>0.207143</td>\n",
       "      <td>3</td>\n",
       "      <td>114.5</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>153</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135</td>\n",
       "      <td>13660</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>513</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>0.010361</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.933540</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>153</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>22174</td>\n",
       "      <td>1</td>\n",
       "      <td>504</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>441</td>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.050564</td>\n",
       "      <td>0.757337</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>3</td>\n",
       "      <td>40.5</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>153</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  order  brand  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0      76   23050      1   1411          4          0         22          0   \n",
       "1     116    9408      1    322          4          0        536          0   \n",
       "2     116   25677      1    322          4          0        536          0   \n",
       "3     135   13660      1    157          4          0        513          0   \n",
       "4     135   22174      1    504         10          0        441          3   \n",
       "\n",
       "   feature_5  brandOrderRatio  feature1OrderRatio  feature2OrderRatio  \\\n",
       "0        151         0.007899            0.466804            0.826492   \n",
       "1        144         0.012288            0.466804            0.826492   \n",
       "2        144         0.012288            0.466804            0.826492   \n",
       "3        137         0.010361            0.466804            0.826492   \n",
       "4         84         0.005653            0.369146            0.826492   \n",
       "\n",
       "   feature3OrderRatio  feature4OrderRatio  feature5OrderRatio  TotalBFscore  \\\n",
       "0            0.008540            0.640224            0.018705      0.940897   \n",
       "1            0.034208            0.640224            0.086085      0.988336   \n",
       "2            0.034208            0.640224            0.086085      0.988336   \n",
       "3            0.004528            0.640224            0.005142      0.933540   \n",
       "4            0.005184            0.334600            0.050564      0.757337   \n",
       "\n",
       "        RCP  TotalItemOrders(user)  MeanDiffToNxt(user)  date(year)  \\\n",
       "0  0.259374                      2                169.0        2020   \n",
       "1  0.040000                      3                114.5        2020   \n",
       "2  0.207143                      3                114.5        2020   \n",
       "3  0.055556                      2                 32.0        2020   \n",
       "4  0.454545                      3                 40.5        2020   \n",
       "\n",
       "   date(month)  date(weekOfMonth)  date(dayOfMonth)  date(weekOfYear)  \\\n",
       "0            6                  1                 1                23   \n",
       "1            6                  1                 1                23   \n",
       "2            6                  1                 1                23   \n",
       "3            6                  1                 1                23   \n",
       "4            6                  1                 1                23   \n",
       "\n",
       "   date(dayOfYear)  nextBuyInWeeks(floor)  \n",
       "0              153                     24  \n",
       "1              153                     22  \n",
       "2              153                     22  \n",
       "3              153                      4  \n",
       "4              153                      4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file, usecols=columns, sep='|', dtype=dtype, nrows=None, converters={'date':pd.to_datetime})\n",
    "\n",
    "show_mem_usage(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbeafb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userID',\n",
       " 'itemID',\n",
       " 'order',\n",
       " 'brand',\n",
       " 'feature_1',\n",
       " 'feature_2',\n",
       " 'feature_3',\n",
       " 'feature_4',\n",
       " 'feature_5',\n",
       " 'brandOrderRatio',\n",
       " 'feature1OrderRatio',\n",
       " 'feature2OrderRatio',\n",
       " 'feature3OrderRatio',\n",
       " 'feature4OrderRatio',\n",
       " 'feature5OrderRatio',\n",
       " 'TotalBFscore',\n",
       " 'RCP',\n",
       " 'TotalItemOrders(user)',\n",
       " 'MeanDiffToNxt(user)',\n",
       " 'date(year)',\n",
       " 'date(month)',\n",
       " 'date(weekOfMonth)',\n",
       " 'date(dayOfMonth)',\n",
       " 'date(weekOfYear)',\n",
       " 'date(dayOfYear)',\n",
       " 'nextBuyInWeeks(floor)']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_headers = list(df.columns)\n",
    "column_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3dddf",
   "metadata": {},
   "source": [
    "## Training & Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57002dd",
   "metadata": {},
   "source": [
    "Pipeline needs training method, dataframe and dates to split dataframe in training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8540e714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0016473 , 0.70337819, 0.        , ..., 0.        , 0.42307692,\n",
       "        0.41643836],\n",
       "       [0.00251431, 0.2870701 , 0.        , ..., 0.        , 0.42307692,\n",
       "        0.41643836],\n",
       "       [0.00251431, 0.78354542, 0.        , ..., 0.        , 0.42307692,\n",
       "        0.41643836],\n",
       "       ...,\n",
       "       [0.466382  , 0.59330465, 0.        , ..., 0.76666667, 0.03846154,\n",
       "        0.0630137 ],\n",
       "       [0.78142882, 0.70636882, 0.        , ..., 0.76666667, 0.03846154,\n",
       "        0.0630137 ],\n",
       "       [0.97409832, 0.03576551, 0.        , ..., 0.76666667, 0.03846154,\n",
       "        0.0630137 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df.iloc[:,:-1]\n",
    "y_train = df.iloc[:,-1]\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "#scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_nrm = scaler_X.fit_transform(X_train)\n",
    "#y_train_nrm = scaler_y.fit_transform(y_train)\n",
    "\n",
    "#df_normalized = pd.DataFrame(df_normalized)\n",
    "#df_normalized.set_axis(column_headers, axis=1,inplace=True)\n",
    "#df_normalized\n",
    "X_train_nrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784730b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>order</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>brandOrderRatio</th>\n",
       "      <th>feature1OrderRatio</th>\n",
       "      <th>feature2OrderRatio</th>\n",
       "      <th>feature3OrderRatio</th>\n",
       "      <th>feature4OrderRatio</th>\n",
       "      <th>feature5OrderRatio</th>\n",
       "      <th>TotalBFscore</th>\n",
       "      <th>RCP</th>\n",
       "      <th>TotalItemOrders(user)</th>\n",
       "      <th>MeanDiffToNxt(user)</th>\n",
       "      <th>date(year)</th>\n",
       "      <th>date(month)</th>\n",
       "      <th>date(weekOfMonth)</th>\n",
       "      <th>date(dayOfMonth)</th>\n",
       "      <th>date(weekOfYear)</th>\n",
       "      <th>date(dayOfYear)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.703378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932584</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042672</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.795812</td>\n",
       "      <td>0.170847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135717</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.217280</td>\n",
       "      <td>0.940897</td>\n",
       "      <td>0.255945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.287070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212822</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996289</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.759162</td>\n",
       "      <td>0.265774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.543643</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988336</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.462656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.783545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212822</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996289</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.759162</td>\n",
       "      <td>0.265774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.543643</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988336</td>\n",
       "      <td>0.203472</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.462656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.416827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103767</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953618</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.722513</td>\n",
       "      <td>0.224108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071964</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.059728</td>\n",
       "      <td>0.933540</td>\n",
       "      <td>0.051183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.676646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820037</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.445026</td>\n",
       "      <td>0.122258</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082380</td>\n",
       "      <td>0.52263</td>\n",
       "      <td>0.587373</td>\n",
       "      <td>0.757337</td>\n",
       "      <td>0.452020</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.155602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171588</th>\n",
       "      <td>0.422273</td>\n",
       "      <td>0.054350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.759162</td>\n",
       "      <td>0.483508</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.197150</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607925</td>\n",
       "      <td>0.122522</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.120332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171589</th>\n",
       "      <td>0.466382</td>\n",
       "      <td>0.018096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134171</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.912801</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.350785</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104368</td>\n",
       "      <td>0.601176</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.419063</td>\n",
       "      <td>0.606568</td>\n",
       "      <td>0.665123</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.084025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171590</th>\n",
       "      <td>0.466382</td>\n",
       "      <td>0.593305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.204188</td>\n",
       "      <td>0.233151</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014103</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.075256</td>\n",
       "      <td>0.885075</td>\n",
       "      <td>0.222834</td>\n",
       "      <td>0.012295</td>\n",
       "      <td>0.112626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171591</th>\n",
       "      <td>0.781429</td>\n",
       "      <td>0.706369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925975</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270872</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.858639</td>\n",
       "      <td>0.077304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029678</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.204021</td>\n",
       "      <td>0.934988</td>\n",
       "      <td>0.247900</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.294606</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171592</th>\n",
       "      <td>0.974098</td>\n",
       "      <td>0.035766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906948</td>\n",
       "      <td>0.062346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171593 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userID    itemID  order     brand  feature_1  feature_2  feature_3  \\\n",
       "0       0.001647  0.703378    0.0  0.932584   0.454545   0.000000   0.042672   \n",
       "1       0.002514  0.287070    0.0  0.212822   0.454545   0.000000   0.996289   \n",
       "2       0.002514  0.783545    0.0  0.212822   0.454545   0.000000   0.996289   \n",
       "3       0.002926  0.416827    0.0  0.103767   0.454545   0.000000   0.953618   \n",
       "4       0.002926  0.676646    0.0  0.333113   1.000000   0.000000   0.820037   \n",
       "...          ...       ...    ...       ...        ...        ...        ...   \n",
       "171588  0.422273  0.054350    0.0  0.003966   0.454545   1.000000   0.597403   \n",
       "171589  0.466382  0.018096    0.0  0.134171   0.454545   0.333333   0.912801   \n",
       "171590  0.466382  0.593305    0.0  0.269663   1.000000   0.000000   0.298701   \n",
       "171591  0.781429  0.706369    0.0  0.925975   0.454545   0.000000   0.270872   \n",
       "171592  0.974098  0.035766    0.0  0.129544   1.000000   0.000000   0.935065   \n",
       "\n",
       "        feature_4  feature_5  brandOrderRatio  feature1OrderRatio  \\\n",
       "0             0.2   0.795812         0.170847            1.000000   \n",
       "1             0.2   0.759162         0.265774            1.000000   \n",
       "2             0.2   0.759162         0.265774            1.000000   \n",
       "3             0.2   0.722513         0.224108            1.000000   \n",
       "4             0.8   0.445026         0.122258            0.790795   \n",
       "...           ...        ...              ...                 ...   \n",
       "171588        0.2   0.759162         0.483508            1.000000   \n",
       "171589        0.2   0.350785         0.009187            1.000000   \n",
       "171590        0.2   0.204188         0.233151            0.790795   \n",
       "171591        0.2   0.858639         0.077304            1.000000   \n",
       "171592        0.2   0.000000         0.003130            0.790795   \n",
       "\n",
       "        feature2OrderRatio  feature3OrderRatio  feature4OrderRatio  \\\n",
       "0                 1.000000            0.135717             1.00000   \n",
       "1                 1.000000            0.543643             1.00000   \n",
       "2                 1.000000            0.543643             1.00000   \n",
       "3                 1.000000            0.071964             1.00000   \n",
       "4                 1.000000            0.082380             0.52263   \n",
       "...                    ...                 ...                 ...   \n",
       "171588            0.050417            0.197150             1.00000   \n",
       "171589            0.104368            0.601176             1.00000   \n",
       "171590            1.000000            0.014103             1.00000   \n",
       "171591            1.000000            0.029678             1.00000   \n",
       "171592            1.000000            1.000000             1.00000   \n",
       "\n",
       "        feature5OrderRatio  TotalBFscore       RCP  TotalItemOrders(user)  \\\n",
       "0                 0.217280      0.940897  0.255945               0.000000   \n",
       "1                 1.000000      0.988336  0.035556               0.002049   \n",
       "2                 1.000000      0.988336  0.203472               0.002049   \n",
       "3                 0.059728      0.933540  0.051183               0.000000   \n",
       "4                 0.587373      0.757337  0.452020               0.002049   \n",
       "...                    ...           ...       ...                    ...   \n",
       "171588            1.000000      0.607925  0.122522               0.010246   \n",
       "171589            0.419063      0.606568  0.665123               0.010246   \n",
       "171590            0.075256      0.885075  0.222834               0.012295   \n",
       "171591            0.204021      0.934988  0.247900               0.004098   \n",
       "171592            0.000000      0.906948  0.062346               0.000000   \n",
       "\n",
       "        MeanDiffToNxt(user)  date(year)  date(month)  date(weekOfMonth)  \\\n",
       "0                  0.688797         0.0     0.454545                0.2   \n",
       "1                  0.462656         0.0     0.454545                0.2   \n",
       "2                  0.462656         0.0     0.454545                0.2   \n",
       "3                  0.120332         0.0     0.454545                0.2   \n",
       "4                  0.155602         0.0     0.454545                0.2   \n",
       "...                     ...         ...          ...                ...   \n",
       "171588             0.120332         1.0     0.000000                0.6   \n",
       "171589             0.084025         1.0     0.000000                0.6   \n",
       "171590             0.112626         1.0     0.000000                0.6   \n",
       "171591             0.294606         1.0     0.000000                0.6   \n",
       "171592             0.016598         1.0     0.000000                0.6   \n",
       "\n",
       "        date(dayOfMonth)  date(weekOfYear)  date(dayOfYear)  \n",
       "0               0.000000          0.423077         0.416438  \n",
       "1               0.000000          0.423077         0.416438  \n",
       "2               0.000000          0.423077         0.416438  \n",
       "3               0.000000          0.423077         0.416438  \n",
       "4               0.000000          0.423077         0.416438  \n",
       "...                  ...               ...              ...  \n",
       "171588          0.766667          0.038462         0.063014  \n",
       "171589          0.766667          0.038462         0.063014  \n",
       "171590          0.766667          0.038462         0.063014  \n",
       "171591          0.766667          0.038462         0.063014  \n",
       "171592          0.766667          0.038462         0.063014  \n",
       "\n",
       "[171593 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.DataFrame(X_train_nrm)\n",
    "X_train.set_axis(column_headers[:-1], axis=1,inplace=True)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6f7469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         24\n",
       "1         22\n",
       "2         22\n",
       "3          4\n",
       "4          4\n",
       "          ..\n",
       "171588     1\n",
       "171589     1\n",
       "171590     1\n",
       "171591     1\n",
       "171592     1\n",
       "Name: nextBuyInWeeks(floor), Length: 171593, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84d8bf93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = XGBRegressor(tree_method='gpu_hist', gpu_id=0)\n",
    "fitted_model = model.fit(X_train, y_train)\n",
    "\n",
    "xgb.plot_importance(model, importance_type='gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce7b10b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171588</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171589</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171590</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171591</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171592</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171593 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0       24\n",
       "1       20\n",
       "2       20\n",
       "3        4\n",
       "4        5\n",
       "...     ..\n",
       "171588   1\n",
       "171589   2\n",
       "171590   2\n",
       "171591   1\n",
       "171592   1\n",
       "\n",
       "[171593 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred = np.array([round(i) for i in y_train_pred]) \n",
    "y_train_pred = pd.DataFrame(y_train_pred)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e4c33ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (25!=1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_train \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print info about accuracies\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m XGboost train \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_train\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\sklearn\\metrics\\_regression.py:442\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_squared_error\u001b[39m(\n\u001b[0;32m    383\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    384\u001b[0m ):\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    446\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\sklearn\\metrics\\_regression.py:111\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m    108\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true and y_pred have different number of output (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m!=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    113\u001b[0m             y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    114\u001b[0m         )\n\u001b[0;32m    115\u001b[0m     )\n\u001b[0;32m    117\u001b[0m n_outputs \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    118\u001b[0m allowed_multioutput_str \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariance_weighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred have different number of output (25!=1)"
     ]
    }
   ],
   "source": [
    "model_train = mean_squared_error(X_train, y_train_pred)\n",
    "\n",
    "# print info about accuracies\n",
    "print(f'\\n XGboost train '\n",
    "     f'{model_train:.3f}')\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1823af06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>order</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>brandOrderRatio</th>\n",
       "      <th>feature1OrderRatio</th>\n",
       "      <th>feature2OrderRatio</th>\n",
       "      <th>feature3OrderRatio</th>\n",
       "      <th>feature4OrderRatio</th>\n",
       "      <th>feature5OrderRatio</th>\n",
       "      <th>TotalBFscore</th>\n",
       "      <th>RCP</th>\n",
       "      <th>TotalItemOrders(user)</th>\n",
       "      <th>MeanDiffToNxt(user)</th>\n",
       "      <th>date(year)</th>\n",
       "      <th>date(month)</th>\n",
       "      <th>date(weekOfMonth)</th>\n",
       "      <th>date(dayOfMonth)</th>\n",
       "      <th>date(weekOfYear)</th>\n",
       "      <th>date(dayOfYear)</th>\n",
       "      <th>nextBuyInWeeks(floor)</th>\n",
       "      <th>nextBuyIn_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.703378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932584</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042672</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.795812</td>\n",
       "      <td>0.170847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135717</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.217280</td>\n",
       "      <td>0.940897</td>\n",
       "      <td>0.255945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.723664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.287070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212822</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996289</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.759162</td>\n",
       "      <td>0.265774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.543643</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988336</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.462656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.555153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.783545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212822</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996289</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.759162</td>\n",
       "      <td>0.265774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.543643</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988336</td>\n",
       "      <td>0.203472</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.462656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.533073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.416827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103767</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953618</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.722513</td>\n",
       "      <td>0.224108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071964</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.059728</td>\n",
       "      <td>0.933540</td>\n",
       "      <td>0.051183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.099386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.676646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820037</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.445026</td>\n",
       "      <td>0.122258</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082380</td>\n",
       "      <td>0.52263</td>\n",
       "      <td>0.587373</td>\n",
       "      <td>0.757337</td>\n",
       "      <td>0.452020</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.155602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.139552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.822088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.905380</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.235602</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384965</td>\n",
       "      <td>0.52263</td>\n",
       "      <td>0.686971</td>\n",
       "      <td>0.816166</td>\n",
       "      <td>0.119811</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.307054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.289302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.223290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782931</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.036649</td>\n",
       "      <td>0.011841</td>\n",
       "      <td>0.326553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.347627</td>\n",
       "      <td>0.52263</td>\n",
       "      <td>0.023258</td>\n",
       "      <td>0.633827</td>\n",
       "      <td>0.276235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.243110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.813086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666048</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.130890</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.151113</td>\n",
       "      <td>0.52263</td>\n",
       "      <td>0.050124</td>\n",
       "      <td>0.735008</td>\n",
       "      <td>0.102302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.260043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005289</td>\n",
       "      <td>0.315542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677462</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.369202</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.094241</td>\n",
       "      <td>0.045373</td>\n",
       "      <td>0.326553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013816</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.916287</td>\n",
       "      <td>0.810581</td>\n",
       "      <td>0.225416</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.439834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.449554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.478074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793787</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057514</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.858639</td>\n",
       "      <td>0.277413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025196</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.204021</td>\n",
       "      <td>0.939354</td>\n",
       "      <td>0.134336</td>\n",
       "      <td>0.012295</td>\n",
       "      <td>0.202282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.172167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.876041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820037</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.445026</td>\n",
       "      <td>0.122258</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082380</td>\n",
       "      <td>0.52263</td>\n",
       "      <td>0.587373</td>\n",
       "      <td>0.757337</td>\n",
       "      <td>0.366647</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.235477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.192328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.007023</td>\n",
       "      <td>0.388813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793787</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.235602</td>\n",
       "      <td>0.277413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025535</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.686971</td>\n",
       "      <td>0.959606</td>\n",
       "      <td>0.143881</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.477178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416438</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.475710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171581</th>\n",
       "      <td>0.323023</td>\n",
       "      <td>0.409991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.768672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430427</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.785340</td>\n",
       "      <td>0.125647</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.019649</td>\n",
       "      <td>0.880308</td>\n",
       "      <td>0.086470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.060274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171582</th>\n",
       "      <td>0.396177</td>\n",
       "      <td>0.905002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.641929</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.445026</td>\n",
       "      <td>0.929316</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776499</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.587373</td>\n",
       "      <td>0.945564</td>\n",
       "      <td>0.280226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.060274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171583</th>\n",
       "      <td>0.501691</td>\n",
       "      <td>0.189783</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.996289</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.759162</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104368</td>\n",
       "      <td>0.543643</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.629187</td>\n",
       "      <td>0.106996</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.060274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171584</th>\n",
       "      <td>0.570379</td>\n",
       "      <td>0.023040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241904</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923933</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.073298</td>\n",
       "      <td>0.107360</td>\n",
       "      <td>0.326553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161104</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.121098</td>\n",
       "      <td>0.783160</td>\n",
       "      <td>0.090147</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.060274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171585</th>\n",
       "      <td>0.797533</td>\n",
       "      <td>0.490036</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.122935</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.643979</td>\n",
       "      <td>0.914548</td>\n",
       "      <td>0.326553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.114283</td>\n",
       "      <td>0.52263</td>\n",
       "      <td>0.325854</td>\n",
       "      <td>0.659677</td>\n",
       "      <td>0.098623</td>\n",
       "      <td>0.012295</td>\n",
       "      <td>0.179806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.060274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171586</th>\n",
       "      <td>0.226851</td>\n",
       "      <td>0.286551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.541744</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.732984</td>\n",
       "      <td>0.257597</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.562153</td>\n",
       "      <td>0.52263</td>\n",
       "      <td>0.036811</td>\n",
       "      <td>0.377310</td>\n",
       "      <td>0.025610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171587</th>\n",
       "      <td>0.226851</td>\n",
       "      <td>0.314260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730983</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.638743</td>\n",
       "      <td>0.176688</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.52263</td>\n",
       "      <td>0.035634</td>\n",
       "      <td>0.733848</td>\n",
       "      <td>0.125779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171588</th>\n",
       "      <td>0.422273</td>\n",
       "      <td>0.054350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.759162</td>\n",
       "      <td>0.483508</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.197150</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607925</td>\n",
       "      <td>0.122522</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.120332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171589</th>\n",
       "      <td>0.466382</td>\n",
       "      <td>0.018096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134171</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.912801</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.350785</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104368</td>\n",
       "      <td>0.601176</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.419063</td>\n",
       "      <td>0.606568</td>\n",
       "      <td>0.665123</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.084025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171590</th>\n",
       "      <td>0.466382</td>\n",
       "      <td>0.593305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.204188</td>\n",
       "      <td>0.233151</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014103</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.075256</td>\n",
       "      <td>0.885075</td>\n",
       "      <td>0.222834</td>\n",
       "      <td>0.012295</td>\n",
       "      <td>0.112626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171591</th>\n",
       "      <td>0.781429</td>\n",
       "      <td>0.706369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925975</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270872</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.858639</td>\n",
       "      <td>0.077304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029678</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.204021</td>\n",
       "      <td>0.934988</td>\n",
       "      <td>0.247900</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.294606</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171592</th>\n",
       "      <td>0.974098</td>\n",
       "      <td>0.035766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906948</td>\n",
       "      <td>0.062346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.063014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.017735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171593 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userID    itemID     order     brand  feature_1  feature_2  \\\n",
       "0       0.001647  0.703378  0.000000  0.932584   0.454545   0.000000   \n",
       "1       0.002514  0.287070  0.000000  0.212822   0.454545   0.000000   \n",
       "2       0.002514  0.783545  0.000000  0.212822   0.454545   0.000000   \n",
       "3       0.002926  0.416827  0.000000  0.103767   0.454545   0.000000   \n",
       "4       0.002926  0.676646  0.000000  0.333113   1.000000   0.000000   \n",
       "5       0.004378  0.822088  0.000000  0.831461   0.454545   0.000000   \n",
       "6       0.005202  0.223290  0.000000  0.882353   0.636364   0.000000   \n",
       "7       0.005202  0.813086  0.000000  0.428288   1.000000   0.000000   \n",
       "8       0.005289  0.315542  0.000000  0.677462   0.636364   0.000000   \n",
       "9       0.005982  0.478074  0.000000  0.793787   0.454545   0.000000   \n",
       "10      0.005982  0.876041  0.000000  0.333113   1.000000   0.000000   \n",
       "11      0.007023  0.388813  0.000000  0.793787   0.454545   0.000000   \n",
       "...          ...       ...       ...       ...        ...        ...   \n",
       "171581  0.323023  0.409991  0.000000  0.768672   1.000000   0.000000   \n",
       "171582  0.396177  0.905002  0.000000  0.011897   1.000000   0.000000   \n",
       "171583  0.501691  0.189783  0.010101  0.797753   0.454545   0.333333   \n",
       "171584  0.570379  0.023040  0.000000  0.241904   0.636364   0.000000   \n",
       "171585  0.797533  0.490036  0.010101  0.122935   0.636364   0.000000   \n",
       "171586  0.226851  0.286551  0.000000  0.744217   1.000000   1.000000   \n",
       "171587  0.226851  0.314260  0.000000  0.148711   1.000000   0.000000   \n",
       "171588  0.422273  0.054350  0.000000  0.003966   0.454545   1.000000   \n",
       "171589  0.466382  0.018096  0.000000  0.134171   0.454545   0.333333   \n",
       "171590  0.466382  0.593305  0.000000  0.269663   1.000000   0.000000   \n",
       "171591  0.781429  0.706369  0.000000  0.925975   0.454545   0.000000   \n",
       "171592  0.974098  0.035766  0.000000  0.129544   1.000000   0.000000   \n",
       "\n",
       "        feature_3  feature_4  feature_5  brandOrderRatio  feature1OrderRatio  \\\n",
       "0        0.042672        0.2   0.795812         0.170847            1.000000   \n",
       "1        0.996289        0.2   0.759162         0.265774            1.000000   \n",
       "2        0.996289        0.2   0.759162         0.265774            1.000000   \n",
       "3        0.953618        0.2   0.722513         0.224108            1.000000   \n",
       "4        0.820037        0.8   0.445026         0.122258            0.790795   \n",
       "5        0.905380        0.8   0.235602         0.026220            1.000000   \n",
       "6        0.782931        0.8   0.036649         0.011841            0.326553   \n",
       "7        0.666048        0.8   0.130890         0.037037            0.790795   \n",
       "8        0.369202        0.2   0.094241         0.045373            0.326553   \n",
       "9        0.057514        0.2   0.858639         0.277413            1.000000   \n",
       "10       0.820037        0.8   0.445026         0.122258            0.790795   \n",
       "11       0.324675        0.2   0.235602         0.277413            1.000000   \n",
       "...           ...        ...        ...              ...                 ...   \n",
       "171581   0.430427        0.2   0.785340         0.125647            0.790795   \n",
       "171582   0.641929        0.2   0.445026         0.929316            0.790795   \n",
       "171583   0.996289        0.2   0.759162         0.010687            1.000000   \n",
       "171584   0.923933        0.2   0.073298         0.107360            0.326553   \n",
       "171585   0.387755        0.8   0.643979         0.914548            0.326553   \n",
       "171586   0.541744        0.8   0.732984         0.257597            0.790795   \n",
       "171587   0.730983        0.8   0.638743         0.176688            0.790795   \n",
       "171588   0.597403        0.2   0.759162         0.483508            1.000000   \n",
       "171589   0.912801        0.2   0.350785         0.009187            1.000000   \n",
       "171590   0.298701        0.2   0.204188         0.233151            0.790795   \n",
       "171591   0.270872        0.2   0.858639         0.077304            1.000000   \n",
       "171592   0.935065        0.2   0.000000         0.003130            0.790795   \n",
       "\n",
       "        feature2OrderRatio  feature3OrderRatio  feature4OrderRatio  \\\n",
       "0                 1.000000            0.135717             1.00000   \n",
       "1                 1.000000            0.543643             1.00000   \n",
       "2                 1.000000            0.543643             1.00000   \n",
       "3                 1.000000            0.071964             1.00000   \n",
       "4                 1.000000            0.082380             0.52263   \n",
       "5                 1.000000            0.384965             0.52263   \n",
       "6                 1.000000            0.347627             0.52263   \n",
       "7                 1.000000            0.151113             0.52263   \n",
       "8                 1.000000            0.013816             1.00000   \n",
       "9                 1.000000            0.025196             1.00000   \n",
       "10                1.000000            0.082380             0.52263   \n",
       "11                1.000000            0.025535             1.00000   \n",
       "...                    ...                 ...                 ...   \n",
       "171581            1.000000            0.013541             1.00000   \n",
       "171582            1.000000            0.776499             1.00000   \n",
       "171583            0.104368            0.543643             1.00000   \n",
       "171584            1.000000            0.161104             1.00000   \n",
       "171585            1.000000            0.114283             0.52263   \n",
       "171586            0.050417            0.562153             0.52263   \n",
       "171587            1.000000            0.030494             0.52263   \n",
       "171588            0.050417            0.197150             1.00000   \n",
       "171589            0.104368            0.601176             1.00000   \n",
       "171590            1.000000            0.014103             1.00000   \n",
       "171591            1.000000            0.029678             1.00000   \n",
       "171592            1.000000            1.000000             1.00000   \n",
       "\n",
       "        feature5OrderRatio  TotalBFscore       RCP  TotalItemOrders(user)  \\\n",
       "0                 0.217280      0.940897  0.255945               0.000000   \n",
       "1                 1.000000      0.988336  0.035556               0.002049   \n",
       "2                 1.000000      0.988336  0.203472               0.002049   \n",
       "3                 0.059728      0.933540  0.051183               0.000000   \n",
       "4                 0.587373      0.757337  0.452020               0.002049   \n",
       "5                 0.686971      0.816166  0.119811               0.002049   \n",
       "6                 0.023258      0.633827  0.276235               0.000000   \n",
       "7                 0.050124      0.735008  0.102302               0.000000   \n",
       "8                 0.916287      0.810581  0.225416               0.002049   \n",
       "9                 0.204021      0.939354  0.134336               0.012295   \n",
       "10                0.587373      0.757337  0.366647               0.006148   \n",
       "11                0.686971      0.959606  0.143881               0.002049   \n",
       "...                    ...           ...       ...                    ...   \n",
       "171581            0.019649      0.880308  0.086470               0.000000   \n",
       "171582            0.587373      0.945564  0.280226               0.000000   \n",
       "171583            1.000000      0.629187  0.106996               0.004098   \n",
       "171584            0.121098      0.783160  0.090147               0.002049   \n",
       "171585            0.325854      0.659677  0.098623               0.012295   \n",
       "171586            0.036811      0.377310  0.025610               0.000000   \n",
       "171587            0.035634      0.733848  0.125779               0.000000   \n",
       "171588            1.000000      0.607925  0.122522               0.010246   \n",
       "171589            0.419063      0.606568  0.665123               0.010246   \n",
       "171590            0.075256      0.885075  0.222834               0.012295   \n",
       "171591            0.204021      0.934988  0.247900               0.004098   \n",
       "171592            0.000000      0.906948  0.062346               0.000000   \n",
       "\n",
       "        MeanDiffToNxt(user)  date(year)  date(month)  date(weekOfMonth)  \\\n",
       "0                  0.688797         0.0     0.454545                0.2   \n",
       "1                  0.462656         0.0     0.454545                0.2   \n",
       "2                  0.462656         0.0     0.454545                0.2   \n",
       "3                  0.120332         0.0     0.454545                0.2   \n",
       "4                  0.155602         0.0     0.454545                0.2   \n",
       "5                  0.307054         0.0     0.454545                0.2   \n",
       "6                  0.282158         0.0     0.454545                0.2   \n",
       "7                  0.282158         0.0     0.454545                0.2   \n",
       "8                  0.439834         0.0     0.454545                0.2   \n",
       "9                  0.202282         0.0     0.454545                0.2   \n",
       "10                 0.235477         0.0     0.454545                0.2   \n",
       "11                 0.477178         0.0     0.454545                0.2   \n",
       "...                     ...         ...          ...                ...   \n",
       "171581             0.020747         1.0     0.000000                0.6   \n",
       "171582             0.020747         1.0     0.000000                0.6   \n",
       "171583             0.020747         1.0     0.000000                0.6   \n",
       "171584             0.016598         1.0     0.000000                0.6   \n",
       "171585             0.179806         1.0     0.000000                0.6   \n",
       "171586             0.016598         1.0     0.000000                0.6   \n",
       "171587             0.016598         1.0     0.000000                0.6   \n",
       "171588             0.120332         1.0     0.000000                0.6   \n",
       "171589             0.084025         1.0     0.000000                0.6   \n",
       "171590             0.112626         1.0     0.000000                0.6   \n",
       "171591             0.294606         1.0     0.000000                0.6   \n",
       "171592             0.016598         1.0     0.000000                0.6   \n",
       "\n",
       "        date(dayOfMonth)  date(weekOfYear)  date(dayOfYear)  \\\n",
       "0               0.000000          0.423077         0.416438   \n",
       "1               0.000000          0.423077         0.416438   \n",
       "2               0.000000          0.423077         0.416438   \n",
       "3               0.000000          0.423077         0.416438   \n",
       "4               0.000000          0.423077         0.416438   \n",
       "5               0.000000          0.423077         0.416438   \n",
       "6               0.000000          0.423077         0.416438   \n",
       "7               0.000000          0.423077         0.416438   \n",
       "8               0.000000          0.423077         0.416438   \n",
       "9               0.000000          0.423077         0.416438   \n",
       "10              0.000000          0.423077         0.416438   \n",
       "11              0.000000          0.423077         0.416438   \n",
       "...                  ...               ...              ...   \n",
       "171581          0.733333          0.038462         0.060274   \n",
       "171582          0.733333          0.038462         0.060274   \n",
       "171583          0.733333          0.038462         0.060274   \n",
       "171584          0.733333          0.038462         0.060274   \n",
       "171585          0.733333          0.038462         0.060274   \n",
       "171586          0.766667          0.038462         0.063014   \n",
       "171587          0.766667          0.038462         0.063014   \n",
       "171588          0.766667          0.038462         0.063014   \n",
       "171589          0.766667          0.038462         0.063014   \n",
       "171590          0.766667          0.038462         0.063014   \n",
       "171591          0.766667          0.038462         0.063014   \n",
       "171592          0.766667          0.038462         0.063014   \n",
       "\n",
       "        nextBuyInWeeks(floor)  nextBuyIn_pred  \n",
       "0                    0.696970        0.723664  \n",
       "1                    0.636364        0.555153  \n",
       "2                    0.636364        0.533073  \n",
       "3                    0.090909        0.099386  \n",
       "4                    0.090909        0.139552  \n",
       "5                    0.303030        0.289302  \n",
       "6                    0.272727        0.243110  \n",
       "7                    0.272727        0.260043  \n",
       "8                    0.424242        0.449554  \n",
       "9                    0.212121        0.172167  \n",
       "10                   0.121212        0.192328  \n",
       "11                   0.424242        0.475710  \n",
       "...                       ...             ...  \n",
       "171581               0.000000       -0.010746  \n",
       "171582               0.000000       -0.009424  \n",
       "171583               0.000000       -0.000806  \n",
       "171584               0.000000       -0.002149  \n",
       "171585               0.000000        0.020365  \n",
       "171586               0.000000        0.013538  \n",
       "171587               0.000000        0.009328  \n",
       "171588               0.000000       -0.005958  \n",
       "171589               0.000000        0.028531  \n",
       "171590               0.000000        0.020647  \n",
       "171591               0.000000        0.043104  \n",
       "171592               0.000000       -0.017735  \n",
       "\n",
       "[171593 rows x 27 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_pred = pd.DataFrame(y_train_pred, columns=['nextBuyIn_pred'], index=X_train.index)\n",
    "# concatenate X, y, y_pred (put columns next to each other)\n",
    "df_eval_train = pd.concat([X_train, y_train, df_y_pred], axis=1)\n",
    "df_eval_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a014ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.0</td>\n",
       "      <td>23050.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1411.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.008540</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.018705</td>\n",
       "      <td>0.940897</td>\n",
       "      <td>0.259374</td>\n",
       "      <td>2.0</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.0</td>\n",
       "      <td>9408.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.034208</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.086085</td>\n",
       "      <td>0.988336</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.0</td>\n",
       "      <td>25677.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.034208</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.086085</td>\n",
       "      <td>0.988336</td>\n",
       "      <td>0.207143</td>\n",
       "      <td>3.0</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135.0</td>\n",
       "      <td>13660.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>513.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.010361</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.933540</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135.0</td>\n",
       "      <td>22174.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.050564</td>\n",
       "      <td>0.757337</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>202.0</td>\n",
       "      <td>26940.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1258.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.059138</td>\n",
       "      <td>0.816166</td>\n",
       "      <td>0.123867</td>\n",
       "      <td>3.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>240.0</td>\n",
       "      <td>7318.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1335.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.152436</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.021874</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.633827</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>240.0</td>\n",
       "      <td>26645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.004315</td>\n",
       "      <td>0.735008</td>\n",
       "      <td>0.106439</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>244.0</td>\n",
       "      <td>10341.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.152436</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.078879</td>\n",
       "      <td>0.810581</td>\n",
       "      <td>0.228986</td>\n",
       "      <td>3.0</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>276.0</td>\n",
       "      <td>15667.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.012826</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.017563</td>\n",
       "      <td>0.939354</td>\n",
       "      <td>0.138325</td>\n",
       "      <td>8.0</td>\n",
       "      <td>51.750000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>276.0</td>\n",
       "      <td>28708.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.050564</td>\n",
       "      <td>0.757337</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>5.0</td>\n",
       "      <td>59.750000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>324.0</td>\n",
       "      <td>12742.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.012826</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.059138</td>\n",
       "      <td>0.959606</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>3.0</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171581</th>\n",
       "      <td>14903.0</td>\n",
       "      <td>13436.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.005810</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.880308</td>\n",
       "      <td>0.090680</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171582</th>\n",
       "      <td>18278.0</td>\n",
       "      <td>29657.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.042962</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.048861</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.050564</td>\n",
       "      <td>0.945564</td>\n",
       "      <td>0.283543</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171583</th>\n",
       "      <td>23146.0</td>\n",
       "      <td>6220.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.100607</td>\n",
       "      <td>0.034208</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.086085</td>\n",
       "      <td>0.629187</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171584</th>\n",
       "      <td>26315.0</td>\n",
       "      <td>756.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.152436</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.010137</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.783160</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171585</th>\n",
       "      <td>36795.0</td>\n",
       "      <td>16059.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.042279</td>\n",
       "      <td>0.152436</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.028051</td>\n",
       "      <td>0.659677</td>\n",
       "      <td>0.102777</td>\n",
       "      <td>8.0</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171586</th>\n",
       "      <td>10466.0</td>\n",
       "      <td>9391.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.011910</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.056881</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>0.377310</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171587</th>\n",
       "      <td>10466.0</td>\n",
       "      <td>10299.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.008169</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>0.733848</td>\n",
       "      <td>0.129808</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171588</th>\n",
       "      <td>19482.0</td>\n",
       "      <td>1782.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.022353</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.056881</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.086085</td>\n",
       "      <td>0.607925</td>\n",
       "      <td>0.126566</td>\n",
       "      <td>7.0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171589</th>\n",
       "      <td>21517.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.100607</td>\n",
       "      <td>0.037829</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.036075</td>\n",
       "      <td>0.606568</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171590</th>\n",
       "      <td>21517.0</td>\n",
       "      <td>19443.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.010779</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.885075</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.142857</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171591</th>\n",
       "      <td>36052.0</td>\n",
       "      <td>23148.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.466804</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.017563</td>\n",
       "      <td>0.934988</td>\n",
       "      <td>0.251366</td>\n",
       "      <td>4.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171592</th>\n",
       "      <td>44941.0</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.369146</td>\n",
       "      <td>0.826492</td>\n",
       "      <td>0.062924</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906948</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171593 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1    2       3     4    5      6    7      8         9   \\\n",
       "0          76.0  23050.0  1.0  1411.0   4.0  0.0   22.0  0.0  151.0  0.007899   \n",
       "1         116.0   9408.0  1.0   322.0   4.0  0.0  536.0  0.0  144.0  0.012288   \n",
       "2         116.0  25677.0  1.0   322.0   4.0  0.0  536.0  0.0  144.0  0.012288   \n",
       "3         135.0  13660.0  1.0   157.0   4.0  0.0  513.0  0.0  137.0  0.010361   \n",
       "4         135.0  22174.0  1.0   504.0  10.0  0.0  441.0  3.0   84.0  0.005653   \n",
       "5         202.0  26940.0  1.0  1258.0   4.0  0.0  487.0  3.0   44.0  0.001213   \n",
       "6         240.0   7318.0  1.0  1335.0   6.0  0.0  421.0  3.0    6.0  0.000549   \n",
       "7         240.0  26645.0  1.0   648.0  10.0  0.0  358.0  3.0   24.0  0.001713   \n",
       "8         244.0  10341.0  1.0  1025.0   6.0  0.0  198.0  0.0   17.0  0.002099   \n",
       "9         276.0  15667.0  1.0  1201.0   4.0  0.0   30.0  0.0  163.0  0.012826   \n",
       "10        276.0  28708.0  1.0   504.0  10.0  0.0  441.0  3.0   84.0  0.005653   \n",
       "11        324.0  12742.0  1.0  1201.0   4.0  0.0  174.0  0.0   44.0  0.012826   \n",
       "...         ...      ...  ...     ...   ...  ...    ...  ...    ...       ...   \n",
       "171581  14903.0  13436.0  1.0  1163.0  10.0  0.0  231.0  0.0  149.0  0.005810   \n",
       "171582  18278.0  29657.0  1.0    18.0  10.0  0.0  345.0  0.0   84.0  0.042962   \n",
       "171583  23146.0   6220.0  2.0  1207.0   4.0  1.0  536.0  0.0  144.0  0.000495   \n",
       "171584  26315.0    756.0  1.0   366.0   6.0  0.0  497.0  0.0   13.0  0.004964   \n",
       "171585  36795.0  16059.0  2.0   186.0   6.0  0.0  208.0  3.0  122.0  0.042279   \n",
       "171586  10466.0   9391.0  1.0  1126.0  10.0  3.0  291.0  3.0  139.0  0.011910   \n",
       "171587  10466.0  10299.0  1.0   225.0  10.0  0.0  393.0  3.0  121.0  0.008169   \n",
       "171588  19482.0   1782.0  1.0     6.0   4.0  3.0  321.0  0.0  144.0  0.022353   \n",
       "171589  21517.0    594.0  1.0   203.0   4.0  1.0  491.0  0.0   66.0  0.000426   \n",
       "171590  21517.0  19443.0  1.0   408.0  10.0  0.0  160.0  0.0   38.0  0.010779   \n",
       "171591  36052.0  23148.0  1.0  1401.0   4.0  0.0  145.0  0.0  163.0  0.003575   \n",
       "171592  44941.0   1173.0  1.0   196.0  10.0  0.0  503.0  0.0   -1.0  0.000146   \n",
       "\n",
       "              10        11        12        13        14        15        16  \\\n",
       "0       0.466804  0.826492  0.008540  0.640224  0.018705  0.940897  0.259374   \n",
       "1       0.466804  0.826492  0.034208  0.640224  0.086085  0.988336  0.040000   \n",
       "2       0.466804  0.826492  0.034208  0.640224  0.086085  0.988336  0.207143   \n",
       "3       0.466804  0.826492  0.004528  0.640224  0.005142  0.933540  0.055556   \n",
       "4       0.369146  0.826492  0.005184  0.334600  0.050564  0.757337  0.454545   \n",
       "5       0.466804  0.826492  0.024224  0.334600  0.059138  0.816166  0.123867   \n",
       "6       0.152436  0.826492  0.021874  0.334600  0.002002  0.633827  0.279570   \n",
       "7       0.369146  0.826492  0.009509  0.334600  0.004315  0.735008  0.106439   \n",
       "8       0.152436  0.826492  0.000869  0.640224  0.078879  0.810581  0.228986   \n",
       "9       0.466804  0.826492  0.001585  0.640224  0.017563  0.939354  0.138325   \n",
       "10      0.369146  0.826492  0.005184  0.334600  0.050564  0.757337  0.369565   \n",
       "11      0.466804  0.826492  0.001607  0.640224  0.059138  0.959606  0.147826   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "171581  0.369146  0.826492  0.000852  0.640224  0.001691  0.880308  0.090680   \n",
       "171582  0.369146  0.826492  0.048861  0.640224  0.050564  0.945564  0.283543   \n",
       "171583  0.466804  0.100607  0.034208  0.640224  0.086085  0.629187  0.111111   \n",
       "171584  0.152436  0.826492  0.010137  0.640224  0.010425  0.783160  0.094340   \n",
       "171585  0.152436  0.826492  0.007191  0.334600  0.028051  0.659677  0.102777   \n",
       "171586  0.369146  0.056881  0.035373  0.334600  0.003169  0.377310  0.030100   \n",
       "171587  0.369146  0.826492  0.001919  0.334600  0.003068  0.733848  0.129808   \n",
       "171588  0.466804  0.056881  0.012406  0.640224  0.086085  0.607925  0.126566   \n",
       "171589  0.466804  0.100607  0.037829  0.640224  0.036075  0.606568  0.666667   \n",
       "171590  0.369146  0.826492  0.000887  0.640224  0.006478  0.885075  0.226415   \n",
       "171591  0.466804  0.826492  0.001867  0.640224  0.017563  0.934988  0.251366   \n",
       "171592  0.369146  0.826492  0.062924  0.640224  0.000000  0.906948  0.066667   \n",
       "\n",
       "         17          18      19   20   21    22    23     24    25  \n",
       "0       2.0  169.000000  2020.0  6.0  1.0   1.0  23.0  153.0  24.0  \n",
       "1       3.0  114.500000  2020.0  6.0  1.0   1.0  23.0  153.0  22.0  \n",
       "2       3.0  114.500000  2020.0  6.0  1.0   1.0  23.0  153.0  22.0  \n",
       "3       2.0   32.000000  2020.0  6.0  1.0   1.0  23.0  153.0   4.0  \n",
       "4       3.0   40.500000  2020.0  6.0  1.0   1.0  23.0  153.0   4.0  \n",
       "5       3.0   77.000000  2020.0  6.0  1.0   1.0  23.0  153.0  11.0  \n",
       "6       2.0   71.000000  2020.0  6.0  1.0   1.0  23.0  153.0  10.0  \n",
       "7       2.0   71.000000  2020.0  6.0  1.0   1.0  23.0  153.0  10.0  \n",
       "8       3.0  109.000000  2020.0  6.0  1.0   1.0  23.0  153.0  15.0  \n",
       "9       8.0   51.750000  2020.0  6.0  1.0   1.0  23.0  153.0   8.0  \n",
       "10      5.0   59.750000  2020.0  6.0  1.0   1.0  23.0  153.0   5.0  \n",
       "11      3.0  118.000000  2020.0  6.0  1.0   1.0  23.0  153.0  15.0  \n",
       "...     ...         ...     ...  ...  ...   ...   ...    ...   ...  \n",
       "171581  2.0    8.000000  2021.0  1.0  3.0  23.0   3.0   23.0   1.0  \n",
       "171582  2.0    8.000000  2021.0  1.0  3.0  23.0   3.0   23.0   1.0  \n",
       "171583  4.0    8.000000  2021.0  1.0  3.0  23.0   3.0   23.0   1.0  \n",
       "171584  3.0    7.000000  2021.0  1.0  3.0  23.0   3.0   23.0   1.0  \n",
       "171585  8.0   46.333333  2021.0  1.0  3.0  23.0   3.0   23.0   1.0  \n",
       "171586  2.0    7.000000  2021.0  1.0  3.0  24.0   3.0   24.0   1.0  \n",
       "171587  2.0    7.000000  2021.0  1.0  3.0  24.0   3.0   24.0   1.0  \n",
       "171588  7.0   32.000000  2021.0  1.0  3.0  24.0   3.0   24.0   1.0  \n",
       "171589  7.0   23.250000  2021.0  1.0  3.0  24.0   3.0   24.0   1.0  \n",
       "171590  8.0   30.142857  2021.0  1.0  3.0  24.0   3.0   24.0   1.0  \n",
       "171591  4.0   74.000000  2021.0  1.0  3.0  24.0   3.0   24.0   1.0  \n",
       "171592  2.0    7.000000  2021.0  1.0  3.0  24.0   3.0   24.0   1.0  \n",
       "\n",
       "[171593 rows x 26 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_denorm = scaler.inverse_transform(df_eval_train.iloc[:,:-1])\n",
    "df_eval_denorm = pd.DataFrame(eval_denorm)\n",
    "df_eval_denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357bb23",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76785623",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6082215e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count of set:\t\t\t\t\t 171593\n",
      "rows where label is not 0:\t\t\t\t 166619 \t (97.101 % of all rows in set)\n",
      "rows where label was predicted correctly AND not 0:\t 0 \t (0.000 % of rows where label is not 0)\n"
     ]
    }
   ],
   "source": [
    "rowcount = len(df_eval_train)\n",
    "should = len(df_eval_train.loc[(df_eval_train['nextBuyInWeeks(floor)'] != 0)])\n",
    "is_ = len(df_eval_train.loc[(df_eval_train['nextBuyInWeeks(floor)'] != 0) & (df_eval_train['nextBuyInWeeks(floor)'] == df_eval_train.nextBuyIn_pred)]) \n",
    "\n",
    "print(f'row count of set:\\t\\t\\t\\t\\t {rowcount}')\n",
    "print(f'rows where label is not 0:\\t\\t\\t\\t {should} \\t ({should/rowcount*100:.3f} % of all rows in set)')\n",
    "print(f'rows where label was predicted correctly AND not 0:\\t {is_} \\t ({is_/should*100:.3f} % of rows where label is not 0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6dcba394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count of set:\t\t\t\t 171593\n",
      "rows where label was predicted correctly:\t 0 \t (0.000 % of rows)\n"
     ]
    }
   ],
   "source": [
    "rowcount = len(df_eval_train)\n",
    "should = rowcount\n",
    "is_ = len(df_eval_train.loc[(df_eval_train['nextBuyInWeeks(floor)'] == df_eval_train.nextBuyIn_pred)]) \n",
    "\n",
    "print(f'row count of set:\\t\\t\\t\\t {rowcount}')\n",
    "print(f'rows where label was predicted correctly:\\t {is_} \\t ({is_/should*100:.3f} % of rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49edaf3d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b1331",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b41be43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataframe...\n",
      "\n",
      "Fitting model...\n",
      "\n",
      "Plotting feature importance for \"gain\". Do not rely on that.\n",
      "\n",
      "https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27\n",
      "\n",
      "Predicting values...\n",
      "\n",
      "\n",
      " XGboost train/test accuracies: 0.020/17.665\n",
      "\n",
      "Executed pipeline.\n",
      "Evaluate with \"evaluate_pred(X, y, y_pred)\"\n",
      "\n",
      "CPU times: total: 3.34 s\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#dtc\n",
    "pred_train, pred_test, X_train, y_train, X_test, y_test = execute_pipeline(train_dtc, df, [\n",
    "    '2020-06-01', # start train set\n",
    "    '2020-10-31', # end train set\n",
    "    '2020-11-01', # start test set\n",
    "    '2020-11-30'  # end test set\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5b7c8",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c038f94",
   "metadata": {},
   "source": [
    "#### train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe11dd52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_eval_train = evaluate_pred(X_train, y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a8888",
   "metadata": {},
   "source": [
    "#### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff3a3876",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_eval_test = evaluate_pred(X_test, y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af791866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count of set:\t\t\t\t\t 130779\n",
      "rows where label is not 0:\t\t\t\t 16091 \t (12.304 % of all rows in set)\n",
      "rows where label was predicted correctly AND not 0:\t 516 \t (3.207 % of rows where label is actually 0)\n"
     ]
    }
   ],
   "source": [
    "rowcount = len(df_eval_test)\n",
    "should = len(df_eval_test.loc[(df_eval_test.nextBuyInWeeks != 0)])\n",
    "is_ = len(df_eval_test.loc[(df_eval_test.nextBuyInWeeks != 0) & (df_eval_test.nextBuyInWeeks == df_eval_test.nextBuyIn_pred)]) \n",
    "\n",
    "print(f'row count of set:\\t\\t\\t\\t\\t {rowcount}')\n",
    "print(f'rows where label is not 0:\\t\\t\\t\\t {should} \\t ({should/rowcount*100:.3f} % of all rows in set)')\n",
    "print(f'rows where label was predicted correctly AND not 0:\\t {is_} \\t ({is_/should*100:.3f} % of rows where label is actually 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5554060",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6ad95ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: six in c:\\users\\leand\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\leand\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages (from hyperopt) (1.22.4)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-2.8.4-py3-none-any.whl (2.0 MB)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\leand\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages (from hyperopt) (1.8.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\leand\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages (from tqdm->hyperopt) (0.4.4)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=e9ee614eafe78c4e768188ae79215eb60d1e7738ab67a895e712001dd53ab0ee\n",
      "  Stored in directory: c:\\users\\leand\\appdata\\local\\pip\\cache\\wheels\\22\\73\\06\\557dc4f4ef68179b9d763930d6eec26b88ed7c389b19588a1c\n",
      "Successfully built future\n",
      "Installing collected packages: tqdm, py4j, networkx, future, cloudpickle, hyperopt\n",
      "Successfully installed cloudpickle-2.1.0 future-0.18.2 hyperopt-0.2.7 networkx-2.8.4 py4j-0.10.9.5 tqdm-4.64.0\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c4e54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "space = {\n",
    "        'eta': hp.quniform('eta', 0.01, 0.5, 0.01),\n",
    "        'max_depth': hp.quniform('max_depth', 3, 5, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40, 180, 1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 1000,\n",
    "        'seed': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b371ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    reg=xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0,\n",
    "                    eta = space['eta'],\n",
    "                    max_depth = int(space['max_depth']), \n",
    "                    gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),\n",
    "                    min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree']),\n",
    "                    n_estimators = space['n_estimators'])\n",
    "    \n",
    "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=evaluation, eval_metric=\"rmse\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "    \n",
    "\n",
    "    pred = reg.predict(X_test)\n",
    "    loss = mean_squared_error(y_test, pred)\n",
    "    print (\"LOSS:\", loss)\n",
    "    return {'loss': loss, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b675577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:                                                                                                                  \n",
      "7.259098485135326                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9393654983792805                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "9.943502864855471                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.937102402336671                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.987377147277132                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.969435708957796                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.960019058418921                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9741734806488935                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.986400245410001                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.777058533830133                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.7788075552264075                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "6.297493190175244                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9550983957579895                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.957335133752035                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.067615600833756                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.063177950203061                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.94136464037826                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.967504752930785                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.003344925474906                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.942459742591774                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.927157463360994                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.947912814453256                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.961545094943043                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.942063201269948                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.95808553257873                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.967030617365036                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9357780316648565                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.939157101661743                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.925788982724434                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.925891215569756                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.933332803987393                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.929536469060589                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.967282218798769                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.982660276707187                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:                                                                                                                  \n",
      "19.334005759740865                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.952195508400986                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.993206689659181                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.961315816329838                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.931472298110829                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.066125841647736                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.965006105004846                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.949863691604532                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.987754378336644                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.052548614067055                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.937246008583604                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9633748477098765                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.980213481051162                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.29031391381684                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.974813243668796                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.974044573569583                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "13.078277172415865                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.9424821662262                                                                                                        \n",
      "LOSS:                                                                                                                  \n",
      "5.999822699335415                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "7.272694397003599                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.176957176389363                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.944413511880377                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.955899447592473                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9511069649204265                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.955169891804681                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.999020743977331                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "8.186963590837568                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.966097425419438                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.977433230298531                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.950081232909797                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.956547002179798                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.968605969112401                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.953963599275315                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.927855179925886                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:                                                                                                                  \n",
      "5.954776784448804                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.956179629222112                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.924005015168141                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.946202522284125                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.93163860534878                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.9513550019566335                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.950760259673164                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.997534545754468                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.948425984063972                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.934848241952014                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.957602237751402                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.961616015778478                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9668122467620694                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.978304178336543                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.948476583499331                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.951341741414343                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.171417244308961                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.931969762235363                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.970570998735901                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.02236218518965                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.949320318510264                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.973133887635014                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.965386117416995                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.490028179335098                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.966409166574447                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.963406709400958                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.944876347176841                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.937539797431486                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.932701258487786                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.966089254925071                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9813299828103945                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.946789156441249                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.105589058329673                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.932227223386182                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:                                                                                                                  \n",
      "5.958166588569138                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.967587800167442                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.950963228265492                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.989573305347509                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.967077578698782                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9684560674759375                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.968342659655545                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.950711287408801                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.948850313917019                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.001530677700923                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.023799576879065                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.963733411975529                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.014932258932177                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.929312714063536                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.757635833757807                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.999453066804                                                                                                         \n",
      "LOSS:                                                                                                                  \n",
      "5.966186357507952                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.957257001938015                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9439205374274025                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "6.065186629037317                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.98853673493851                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.939450621358598                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.982266887900392                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.925137597614373                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.949958304201077                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.936099195998916                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.93988469941221                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.944891846633414                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9321583615885825                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.944794323682947                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.934577795865306                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.945757192000339                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.968358052701855                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.968979092078451                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:                                                                                                                  \n",
      "5.937861075499296                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.98305549708962                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.9341355694397615                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.9351789339826695                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.971123229552376                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9598162788173985                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.937042071717937                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.959968047754373                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.935261374947656                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.93420970204143                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.930491104076898                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.939611481775809                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.930820502712775                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.954171697526475                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.935738471342909                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9432679859605475                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.9360226067838955                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.952572553799311                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.966805831643774                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.954998053315433                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.946058973816308                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.948224355085579                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.049769229406925                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.937659239571978                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9396617713619015                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.941573547479346                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.974746896026528                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.935968923028618                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.986984626468566                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.96061853109362                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.941933034316646                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.943251700999983                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9483050410919525                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "6.021193870491678                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:                                                                                                                  \n",
      "8.169121153078605                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.002713380037092                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9357353685296195                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.956078430277108                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.966438479319146                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.171160911825012                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.0358698791742125                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.9236087218599245                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.962244864610628                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.932014274142378                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.929765636750932                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.927728569419146                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.970685945557945                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9319726886838655                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.932649255244247                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.958961535595789                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.925687913175112                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.934988190027314                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.967815024349342                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.931463901953493                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.930582009371892                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.944009359110916                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.937702684111508                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.931987371894277                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.964997413001724                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.93116337251703                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.961841094215582                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.929168879500656                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.019521729733953                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.960191434184654                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.942733891898542                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.93082821245731                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "6.008859936009423                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.993790501537103                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:                                                                                                                  \n",
      "5.943000912774027                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.946918691172277                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.927484778548702                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.93293682503515                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.929554367709078                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9326219575042645                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.935282615536612                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.940974311614618                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "6.001193970964764                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.939271146960054                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.960609342905219                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.9295496335725595                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "6.018711863069363                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.937696323926359                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.922228857317215                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.934032405336175                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.94790856381315                                                                                                       \n",
      "LOSS:                                                                                                                  \n",
      "5.9310557388882                                                                                                        \n",
      "LOSS:                                                                                                                  \n",
      "5.9469319758611014                                                                                                     \n",
      "LOSS:                                                                                                                  \n",
      "5.974210019961926                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.967185211483217                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.927616030097048                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.936919066457174                                                                                                      \n",
      "LOSS:                                                                                                                  \n",
      "5.939314516823138                                                                                                      \n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 228/1000 [09:01<30:33,  2.37s/trial, best loss: 5.922228857317215]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[1;32m----> 3\u001b[0m best_hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(space)\u001b[0m\n\u001b[0;32m      2\u001b[0m reg\u001b[38;5;241m=\u001b[39mxgb\u001b[38;5;241m.\u001b[39mXGBRegressor(tree_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_hist\u001b[39m\u001b[38;5;124m'\u001b[39m, gpu_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      3\u001b[0m                 eta \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      4\u001b[0m                 max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m                 colsample_bytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      9\u001b[0m                 n_estimators \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m [( X_train, y_train), ( X_test, y_test)]\n\u001b[1;32m---> 13\u001b[0m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrmse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m pred \u001b[38;5;241m=\u001b[39m reg\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, pred)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\xgboost\\core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    531\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\xgboost\\sklearn.py:961\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    956\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    958\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m    959\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m    960\u001b[0m )\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\xgboost\\core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    531\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conda-pip-env\\lib\\site-packages\\xgboost\\core.py:1733\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(dtrain)\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1733\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1737\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 1000,\n",
    "                        trials = trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86202ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are :  \n",
      "\n",
      "{'colsample_bytree': 0.7567134474205279, 'eta': 0.05, 'gamma': 2.8721332723619244, 'max_depth': 5.0, 'min_child_weight': 8.0, 'reg_alpha': 171.0, 'reg_lambda': 0.32452326405309345}\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b05f754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90073629",
   "metadata": {},
   "source": [
    "## XGBoost mit optimierten Hyperparametern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b19f644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataframe...\n",
      "\n",
      "Fitting model...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fitted_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36mexecute_pipeline\u001b[1;34m(train_method, df, list_of_four_df_boundaries)\u001b[0m\n\u001b[0;32m    187\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m dt_train_test_split(df, b[\u001b[38;5;241m0\u001b[39m], b[\u001b[38;5;241m1\u001b[39m], b[\u001b[38;5;241m2\u001b[39m], b[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# make predictions\u001b[39;00m\n\u001b[0;32m    193\u001b[0m pred_train, pred_test \u001b[38;5;241m=\u001b[39m predict_values(model, X_train, y_train, X_test, y_test)\n",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36mtrain_xgb_bestHyper\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     97\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBRegressor(tree_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_hist\u001b[39m\u001b[38;5;124m'\u001b[39m, gpu_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     98\u001b[0m                 eta \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     99\u001b[0m                 max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 min_child_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_child_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m    103\u001b[0m                 colsample_bytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m    107\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m [( X_train, y_train), ( X_test, y_test)]\n\u001b[1;32m--> 109\u001b[0m \u001b[43mfitted_model\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_train, y_train,\n\u001b[0;32m    110\u001b[0m         eval_set\u001b[38;5;241m=\u001b[39mevaluation, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    111\u001b[0m         early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlotting feature importance for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgain\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Do not rely on that.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fitted_model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_train, pred_test, X_train, y_train, X_test, y_test = execute_pipeline(train_xgb_bestHyper, df, [\n",
    "    '2020-06-01', # start train set\n",
    "    '2020-10-31', # end train set\n",
    "    '2020-11-01', # start test set\n",
    "    '2020-11-30'  # end test set\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099976f7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c3469",
   "metadata": {},
   "source": [
    "#### train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f138520a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_eval_train = evaluate_pred(X_train, y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1374803",
   "metadata": {},
   "source": [
    "#### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7c9fb7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_eval_test = evaluate_pred(X_test, y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee8f40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count of set:\t\t\t\t\t 130779\n",
      "rows where label is not 0:\t\t\t\t 16091 \t (12.304 % of all rows in set)\n",
      "rows where label was predicted correctly AND not 0:\t 516 \t (3.207 % of rows where label is actually 0)\n"
     ]
    }
   ],
   "source": [
    "rowcount = len(df_eval_test)\n",
    "should = len(df_eval_test.loc[(df_eval_test.nextBuyInWeeks != 0)])\n",
    "is_ = len(df_eval_test.loc[(df_eval_test.nextBuyInWeeks != 0) & (df_eval_test.nextBuyInWeeks == df_eval_test.nextBuyIn_pred)]) \n",
    "\n",
    "print(f'row count of set:\\t\\t\\t\\t\\t {rowcount}')\n",
    "print(f'rows where label is not 0:\\t\\t\\t\\t {should} \\t ({should/rowcount*100:.3f} % of all rows in set)')\n",
    "print(f'rows where label was predicted correctly AND not 0:\\t {is_} \\t ({is_/should*100:.3f} % of rows where label is actually 0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a191c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7703225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d0724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f004589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9693e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a388fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e8fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4931b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1752732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ac9eec8b",
   "metadata": {},
   "source": [
    "Pipeline needs training method, dataframe and dates to split dataframe in training and test set."
   ]
  },
  {
   "cell_type": "raw",
   "id": "eaf9891d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%%time\n",
    "pred_train, pred_test, X_train, y_train, X_test, y_test = execute_pipeline(train_xgb, df, [\n",
    "    '2020-06-01', # start train set\n",
    "    '2020-10-31', # end train set\n",
    "    '2020-11-01', # start test set\n",
    "    '2020-11-30'  # end test set\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22b0514e",
   "metadata": {},
   "source": [
    "model_train_precision = precision_score(y_train, pred_train, average='macro')\n",
    "model_test_precision = precision_score(y_test, pred_test, average='macro')\n",
    "model_train_precision, model_test_precision"
   ]
  },
  {
   "cell_type": "raw",
   "id": "149a98ed",
   "metadata": {},
   "source": [
    "model_train_precision = balanced_accuracy_score(y_train, pred_train)\n",
    "model_test_precision = balanced_accuracy_score(y_test, pred_test)\n",
    "model_train_precision, model_test_precision"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed81f26c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e01bca7",
   "metadata": {},
   "source": [
    "### train set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8afab6ef",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df_eval_train = evaluate_pred(X_train, y_train, pred_train)\n",
    "df_eval_train.head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71c5eb19",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f3912fc",
   "metadata": {},
   "source": [
    "df_eval_test = evaluate_pred(X_test, y_test, pred_test)\n",
    "df_eval_test.head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31120472",
   "metadata": {},
   "source": [
    "rowcount = len(df_eval_test)\n",
    "should = len(df_eval_test.loc[(df_eval_test.nextBuyInDays != 0)])\n",
    "is_ = len(df_eval_test.loc[(df_eval_test.nextBuyInDays != 0) & (df_eval_test.nextBuyInDays == df_eval_test.nextBuyIn_pred)]) \n",
    "\n",
    "print(f'row count of set:\\t\\t\\t\\t\\t {rowcount}')\n",
    "print(f'rows where label is not 0:\\t\\t\\t\\t {should} \\t ({should/rowcount*100:.3f} % of all rows in set)')\n",
    "print(f'rows where label was predicted correctly AND not 0:\\t {is_} \\t ({is_/should*100:.3f} % of rows where label is not 0)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
